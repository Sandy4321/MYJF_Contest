{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   0.   0.  33.]\n",
      " [  0.   1.   0.  12.]\n",
      " [  0.   0.   1.  18.]]\n",
      "['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"特征提升\"\"\"\n",
    "#原始数据的种类有很多种，除了数字化的信号数据，还有大量符号化的文本，然而，我们无法直接将符号化的文字本身用于计算任务,而是需要\n",
    "#通过某些处理手段，预先将文本量化为特征向量\n",
    "#有些用符号表示的特征已经相对结构化，并且以文字这种数据结构进行存储，这时我们使用DictVectorizer对特征进行抽取和向量化，比如：\n",
    "import pandas as pd\n",
    "measurements = [\n",
    "    {'city':'Dubai','temperature':33},\n",
    "    {'city':'London','temperature':12},\n",
    "    {'city':'San Francisco','temperature':18}\n",
    "]\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "print(vec.fit_transform(measurements).toarray())\n",
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meituan_sxw/Anaconda/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accruacy of Naive Bayes is  0.839770797963\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.86      0.86      0.86       201\n",
      "           comp.graphics       0.59      0.86      0.70       250\n",
      " comp.os.ms-windows.misc       0.89      0.10      0.17       248\n",
      "comp.sys.ibm.pc.hardware       0.60      0.88      0.72       240\n",
      "   comp.sys.mac.hardware       0.93      0.78      0.85       242\n",
      "          comp.windows.x       0.82      0.84      0.83       263\n",
      "            misc.forsale       0.91      0.70      0.79       257\n",
      "               rec.autos       0.89      0.89      0.89       238\n",
      "         rec.motorcycles       0.98      0.92      0.95       276\n",
      "      rec.sport.baseball       0.98      0.91      0.95       251\n",
      "        rec.sport.hockey       0.93      0.99      0.96       233\n",
      "               sci.crypt       0.86      0.98      0.91       238\n",
      "         sci.electronics       0.85      0.88      0.86       249\n",
      "                 sci.med       0.92      0.94      0.93       245\n",
      "               sci.space       0.89      0.96      0.92       221\n",
      "  soc.religion.christian       0.78      0.96      0.86       232\n",
      "      talk.politics.guns       0.88      0.96      0.92       251\n",
      "   talk.politics.mideast       0.90      0.98      0.94       231\n",
      "      talk.politics.misc       0.79      0.89      0.84       188\n",
      "      talk.religion.misc       0.93      0.44      0.60       158\n",
      "\n",
      "             avg / total       0.86      0.84      0.82      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#另外一些文本数据更为直接，几乎没有使用特殊的数据结构进行存储，只是一系列字符串，我们处理这些数据，比较长哟昂的文本特征表示\n",
    "#方法是词袋法：顾名思义，不考虑词语出现的顺序，只是将训练文本中的每个出现过的词汇单独视作一系列特征。我们称这些不重复的词汇集合为词表\n",
    "#于是每条训练文本都可以在高纬度的词表上映射出一个向量，而特征数值的计算常见有两种方法：CountVectorizer和TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25,random_state=33)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "X_count_train = vec.fit_transform(X_train)\n",
    "X_count_test = vec.transform(X_test)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_count_train,y_train)\n",
    "y_count_predict = mnb.predict(X_count_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('The accruacy of Naive Bayes is ',mnb.score(X_count_test,y_test))\n",
    "print(classification_report(y_test,y_count_predict,target_names=news.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accruacy of Naive Bayes is  0.846349745331\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.84      0.67      0.75       201\n",
      "           comp.graphics       0.85      0.74      0.79       250\n",
      " comp.os.ms-windows.misc       0.82      0.85      0.83       248\n",
      "comp.sys.ibm.pc.hardware       0.76      0.88      0.82       240\n",
      "   comp.sys.mac.hardware       0.94      0.84      0.89       242\n",
      "          comp.windows.x       0.96      0.84      0.89       263\n",
      "            misc.forsale       0.93      0.69      0.79       257\n",
      "               rec.autos       0.84      0.92      0.88       238\n",
      "         rec.motorcycles       0.98      0.92      0.95       276\n",
      "      rec.sport.baseball       0.96      0.91      0.94       251\n",
      "        rec.sport.hockey       0.88      0.99      0.93       233\n",
      "               sci.crypt       0.73      0.98      0.83       238\n",
      "         sci.electronics       0.91      0.83      0.87       249\n",
      "                 sci.med       0.97      0.92      0.95       245\n",
      "               sci.space       0.89      0.96      0.93       221\n",
      "  soc.religion.christian       0.51      0.97      0.67       232\n",
      "      talk.politics.guns       0.83      0.96      0.89       251\n",
      "   talk.politics.mideast       0.92      0.97      0.95       231\n",
      "      talk.politics.misc       0.98      0.62      0.76       188\n",
      "      talk.religion.misc       0.93      0.16      0.28       158\n",
      "\n",
      "             avg / total       0.87      0.85      0.84      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_tfidf_train = tfidf_vec.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf_vec.transform(X_test)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_tfidf_train,y_train)\n",
    "y_tfidf_predict = mnb.predict(X_tfidf_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('The accruacy of Naive Bayes is ',mnb.score(X_tfidf_test,y_test))\n",
    "print(classification_report(y_test,y_tfidf_predict,target_names=news.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n"
     ]
    }
   ],
   "source": [
    "\"\"\"特征筛选\"\"\"\n",
    "#特征筛选与PCA这类通过选择主成分对特征进行重建的方法略有区别，对于PCA而言，我们经常无法解释重建之后的特征，\n",
    "#但是特征筛选不存在对特征值的修改，而是更加侧重于寻找那些对模型性能提升较大的少量特征\n",
    "titanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')\n",
    "y = titanic['survived']\n",
    "X = titanic.drop(['row.names','name','survived'],axis=1)\n",
    "X['age'].fillna(X['age'].mean(),inplace=True)\n",
    "X.fillna('UNKNOWN',inplace=True)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=33)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(X_train.to_dict(orient='record'))\n",
    "X_test = vec.transform(X_test.to_dict(orient='record'))\n",
    "print (len(vec.feature_names_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82370820668693012"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(X_train,y_train)\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81762917933130697"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import feature_selection\n",
    "fs = feature_selection.SelectPercentile(feature_selection.chi2,percentile=20)\n",
    "X_train_fs = fs.fit_transform(X_train,y_train)\n",
    "dt.fit(X_train_fs,y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "dt.score(X_test_fs,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85063904  0.85673057  0.87501546  0.88622964  0.86893424  0.87099567\n",
      "  0.87099567  0.875067    0.86894455  0.86896516  0.86691404  0.86796537\n",
      "  0.86694496  0.86486291  0.86384251  0.86891363  0.86892393  0.86689342\n",
      "  0.86995465  0.8608019   0.86487322  0.8608019   0.86893424  0.87098536\n",
      "  0.86690373  0.86487322  0.86693465  0.86893424  0.86589363  0.86794475\n",
      "  0.86995465  0.86589363  0.86688312  0.87097506  0.87097506  0.87606679\n",
      "  0.87097506  0.87197485  0.86692435  0.87199546  0.87098536  0.86587302\n",
      "  0.86284271  0.8598021   0.86691404  0.86998557  0.86083282  0.86386312\n",
      "  0.86082251  0.86286333]\n",
      "3\n",
      "Optimal number of features  7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "percentiles = range(1,100,2)\n",
    "results = []\n",
    "for i in percentiles:\n",
    "    fs = feature_selection.SelectPercentile(feature_selection.chi2,percentile=i)\n",
    "    X_train_fs = fs.fit_transform(X_train,y_train)\n",
    "    scores = cross_val_score(dt,X_train_fs,y_train,cv=5)\n",
    "    results = np.append(results,scores.mean())\n",
    "print (results)\n",
    "#找到最佳的百分比\n",
    "opt = np.where(results==results.max())[0][0]\n",
    "print(opt)\n",
    "print('Optimal number of features ',percentiles[opt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859791795506\n"
     ]
    }
   ],
   "source": [
    "\"\"\"交叉验证\"\"\"\n",
    "#在真正的机器学习平台行实践机器学习任务时就会发现，我们只可以提交预测结果，不能知晓正确答案，\n",
    "#这就要求我们充分使用现有数据，通常的做法时对现有数据进行采样分割，一部分用于模型参数训练，一步峰用于调优模型配置和特征选择，并且对\n",
    "#位置的测试性能作出估计，叫做验证集（Validation），根据验证流程复杂度不同，模型检验方式分为留一验证和交叉验证\n",
    "titanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')\n",
    "y = titanic['survived']\n",
    "X = titanic.drop(['row.names','name','survived'],axis=1)\n",
    "X['age'].fillna(X['age'].mean(),inplace=True)\n",
    "X.fillna('UNKNOWN',inplace=True)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=33)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(X_train.to_dict(orient='record'))\n",
    "X_test = vec.transform(X_test.to_dict(orient='record'))\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "fs = feature_selection.SelectPercentile(feature_selection.chi2,percentile=i)\n",
    "X_train_fs = fs.fit_transform(X_train,y_train)\n",
    "scores = cross_val_score(dt,X_train_fs,y_train,cv=5)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter values for parameter (vect__analyzer) need to be a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-14f8f2c7795a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#n_jobs=-1代表使用计算机的全部CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time _=gs.fit(X_train,y_train)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meituan_sxw/Anaconda/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator, param_grid, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    810\u001b[0m             refit, cv, verbose, pre_dispatch, error_score)\n\u001b[1;32m    811\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0m_check_param_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meituan_sxw/Anaconda/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_check_param_grid\u001b[0;34m(param_grid)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 raise ValueError(\"Parameter values for parameter ({0}) need \"\n\u001b[0;32m--> 348\u001b[0;31m                                  \"to be a sequence.\".format(name))\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter values for parameter (vect__analyzer) need to be a sequence."
     ]
    }
   ],
   "source": [
    "\"\"\"网格搜索\"\"\"\n",
    "#一般情况下模型有许多参数需要配置，这些参数我们一般统称为模型的超参数，如K近邻中的K值，支持向量机中的不同的核函数等等，\n",
    "#多数情况下，超参数的选择是无限的，因此在有限的时间内，除了可以验证人工预设几种超参数组合外，也可以通过启发式的搜索方法对超参数组合进行调优\n",
    "#这种启发式的超参数搜索方法称为网格搜索\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(news.data[:3000],news.target[:3000],test_size=0.25,random_state=33)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X_count_train = vec.fit_transform(X_train)\n",
    "X_count_test = vec.transform(X_test)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "#使用pipeline简化系统搭建流程，将文本抽取与分类器模型串联起来\n",
    "clf = Pipeline([\n",
    "    ('vect',TfidfVectorizer(stop_words='english')),('svc',SVC())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'svc__gamma':np.logspace(-2,1,4),\n",
    "    'svc__C':np.logspace(-1,1,3),\n",
    "    'vect__analyzer':[])\n",
    "}\n",
    "\n",
    "#n_jobs=-1代表使用计算机的全部CPU\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "gs = GridSearchCV(clf,parameters,verbose=2,refit=True,cv=3,n_jobs=-1)\n",
    "\n",
    "%time _=gs.fit(X_train,y_train)\n",
    "print (gs.best_params_,gs.best_score_)\n",
    "print (gs.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
